---
title: 主成分分析详解
date: 2022-01-05 17:37:45
tags:
mathjax: true
---

# 1、主成分分析概要

基础知识点：默认情况下在二维或三维直角坐标系里任何矢量的始点为坐标原点，终点即为坐标上的某点。

特殊情况1：数据集(本文数据集均指Tabular Data)只有一维特征的情况，此时的样本均由一个标量表示，只要在一维坐标系下就可以表示所有样本点。

特殊情况2：当数据集特征只有两个维度或三个维度的时候，此时样本均由二维矢量或三维矢量表示，由基础知识点知道此时的所有样本均可以在二维或三维的直角坐标系里用坐标点的形式表示出来，这些坐标点就是样本点。

一般情况：一般情况下数据集特征远远大于3个维度，在用机器学习算法建模的时候基于各种原因经常需要把数据降到一个较低的维度，同时又要保证丢失的信息尽可能的少，如果丢失的信息量在可忍受的范围内可以降低到三维或二维则可以很容易把样本点的分布在二维或三维的直角坐标系中展示出来，这样可视化样本点分布对数据的理解有很大的帮助。而主成分分析(Principal Components Analysis, PCA)就是达到降维目的的降维算法之一。

主成分分析算法的输入就是原始数据集的整个特征空间的数据，输出的每一个主成分都是其所有输入特征的线性映射，并且所有主成分两两之间相关性为0，而且所有主成分根据方差的不同有一个排序，方差最大的称为第一主成分，其次称为第二主成分，以此类推。一般来说随着数据维度的减小，数据信息会有所损失，主成分分析就是为了尽可能在信息损失最少的情况下对数据集进行降维，具体的降维操作其实就是把排名靠后的若干个主成分直接去掉，也就是说如果经过主成分分析算法变换后保留所有主成分，则不会有信息损失，但是这样维度就没有发生变化。主成分分析在特征工程中属于特征提取操作，其本质是把一个特征空间映射到了另外一个特征空间。

# 2、主成分分析数学原理

由于样本可以以矢量形式唯一表示，那么样本点在不同的坐标系下的样本点坐标则不一样，因为不同的坐标系原点不一样。主成分分析的本质就是把在原始坐标空间的样本点映射到另外一个坐标空间。

假设在原始坐标空间的一个样本矢量为$\vec{x}$，这个样本映射到另外一个坐标空间的样本矢量为$\vec{x'}$，如果新坐标的原点在原始坐标空间中对应的矢量为$\vec{\mu}$，显然有
$$
\vec{x} = \vec{\mu} + \vec{x'}
$$
分别将矢量$\vec{x}$和$\vec{x'}$写成两个坐标系下的坐标形式：
$$
\begin{aligned}
\vec{x} &= \left(x_1,\cdots,x_d\right)^T \\
\vec{x'} &= \left( a_1,\cdots,a_d\right)^T
\end{aligned}
$$
则有
$$
\vec{x} = \vec\mu + \sum_{i=1}^{d}a_i\vec{e_i}
$$
其中$e_i$为新坐标系下的单位基矢量。对上式进行变换推导可得：
$$
\begin{aligned}
\sum_{i=1}^{d}a_i\vec{e_i} &= \vec{x} - \vec\mu \\
\vec{e_i}^T\sum_{i=1}^{d}a_i\vec{e_i} &= \vec{e_i}^T\left(\vec{x} - \vec\mu\right) \\
\vec{e_i}^Ta_i\vec{e_i} &= \vec{e_i}^T\left(\vec{x} - \vec\mu\right) \\
a_i &= \vec{e_i}^T\left(\vec{x} - \vec\mu\right)
\end{aligned}
$$
然而如果只保留新坐标系下$d'<d$个元素，然后用保留的$d'$个元素来恢复原坐标系下的$d$维矢量：
$$
\hat{\vec{x}} = \vec\mu + \sum_{i=1}^{d'}a_i\vec{e_i}
$$
显然$\hat{\vec{x}}$只是对$\vec{x}$的近似，用$\hat{\vec{x}}$来代替$\vec{x}$就会出现一定的误差。在主成分分析算法中，新坐标系的原点选择在样本集$D=\{\vec{x_1},\cdots,\vec{x_n}\}$的均值矢量$\vec\mu$上，然后寻找一组最优的基矢量$\{\vec{e_1},\cdots,\vec{e_d}\}$，使得在只保留$d'$个元素的条件下，由新的坐标恢复样本集$D$的均方误差最小，即求解如下的优化问题：
$$
min\frac{1}{n}\sum_{k=1}^n\Vert\vec{x}_k-\hat{\vec{x}}_k\Vert^2
$$
如果用$a_{ki}$表示第$k$个样本在新坐标系下的第$i$维特征，则可以得到
$$
\begin{aligned}
\vec{x_k} - \hat{\vec{x}}_k &= \left(\vec\mu+\sum_{i=1}^{d}a_{ki}\vec{e_i}\right) - \left(\vec\mu+\sum_{i=1}^{d'}a_{ki}\vec{e_i}\right) \\
&= \sum_{i=d'+1}^da_{ki}\vec{e_i}
\end{aligned}
$$
所以
$$
\begin{aligned}
\frac{1}{n}\sum_{k=1}^n\Vert\vec{x}_k-\hat{\vec{x}}_k\Vert^2 &= \frac{1}{n}\sum_{k=1}^n\Vert\sum_{i=d'+1}^da_{ki}\vec{e_i}\Vert^2 \\
&= \frac{1}{n}\sum_{k=1}^n\left(\sum_{i=d'+1}^da_{ki}\vec{e_i}\right)^T\left(\sum_{i=d'+1}^da_{ki}\vec{e_i}\right) \\
&= \frac{1}{n}\sum_{k=1}^n\sum_{i=d'+1}^da_{ki}^2 \\
&= 
\end{aligned}
$$
